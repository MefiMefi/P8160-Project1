---
title: "A simulation study of comparing three survival models"
author: "Yijing Tao; Renjie Wei; Jibei Zheng; Haolin Zhong; Anyu Zhu"
output: pdf_document
header-includes:
- \usepackage{placeins}
- \usepackage{caption}
---

```{r setup, include=FALSE}
library(tidyverse)
library(caret)
library(survival)
library(fastmap)    
```

## Objective  
In survival analysis, which aims to investigate the efficacy of a treatment 
$X$ on a survival time $T$, the three most important models are proportional hazards 
model of Exponential, Weibull and Cox. 

In proportional hazard models, the hazard ratio, which refers to the 
instantaneous risk of failure at time $t$ giving that a patient has survived until 
time $t$, is defined as:

$$h_i(t) = h_0(t) \exp(x_i \theta)$$
The formula suggests that the hazard ratio is dominated by the baseline hazard 
function $h_0(t)$, a binary treatment indicator $x_i$ which coded 0 for control 
and 1 for the treatment, and our parameter of interest, $\theta$, which is the 
log hazard ratio for the treatment effect and measures the relative hazard 
reduction due to treatment in comparison to the control.

The three proportional hazard models have different assumptions on the baseline 
hazard function, which makes them differ in flexibility and performance. To examine 
their accuracy and efficiency in a series of scenarios and their robustness against
misspecified distribution, we conducted this simulation study.

## Statistical Methods
### Structure of Simulated Survival Data 

In this study, we simulate right censored survival data with one binary treatment indicator $x$. Our response variable is a dichotomous variable, coded as $1$ when event occurred or $0$ when event did not occur during the 5-year observation period. 

Follow up time is measured from time zero until the event occurs, the study ends or the participant is lost, whichever comes first.


### Methods of Survival Analysis

Suppose $T\in[0, \infty)$ is the time to a event of interest, such as death, disease onset, device failure, etc. To analyze such data, we define a survival function $S$ as
$$S(t) = \operatorname{Pr}(T > t) = \int_t^\infty f(s) ds$$

It measures the probability of survive beyond time $t$. If $T$ is the time to death, then $S(t)$ is the probability of living longer than $t$.  A closely-related concept, hazard function $h$, is defined as
$$h(t) = \lim_{\Delta t \rightarrow 0}\frac{\operatorname{Pr}(T \in (t, t + \Delta t) \vert T > t)}{\Delta t} = \frac{f(t)}{S(t)}.$$
where $f(t)$ is the density function of $T$. The hazard function measures the instantaneous risk of failure at time $t$ giving that a patient has survived until time $t$. 

Because of censoring in survival data, in our study, the proportional hazards models are used to investigate the efficacy of a treatment ($X$) on a survival time $T$ through the hazard functions. The proportional hazards model (with a single binary treatment effect) is then given by 
$$h_i(t) = h_0(t) exp(x_i\beta),$$
where $t$ is the time, $h_0(t)$ is the baseline hazard function, $x_i$ is a binary treatment indicator variable coded 0 for control and 1 for the treatment, $\beta$ is the parameter of interest, which is the log hazard ratio for the treatment effect. $\beta$ measures the relative hazard reduction due to treatment in comparison to the control.

We fit the following three models with simulated data generated by using different distribution of time. 

An exponential proportional-hazards model assumes the baseline hazard function is a constant $$h_0(t) = \lambda$$ 
A Weibull proportional-hazards model assumes the baseline hazard function follows Weibull distribution, where $$h_0(t) = \lambda\gamma t^{\gamma-1}$$ for $\gamma>0$ 

A Cox proportional-hazards model leaves $h_0(t)$ unspecified.

### Distribution of Time

Besides Exponential and Weibull distribution, we also used Gompertz distribution and its combination with exponential distribution as baseline hazard functions in order to evaluate the robustness of these three models, characteristics of these distributions is shown by the table below

\begin{center}
\captionof{table}{Characteristic of Exponential, Weibull and Gompertz distributions}
\begin{tabular}{|l|l|l|l|}
\hline
 & Exponential & Weibull & Gompertz \\ \hline
Parameter & $\lambda >0$ & $\lambda >0$, $\gamma >0$ & $\lambda >0$, $\alpha \in (-\infty,\infty)$ \\ \hline
Hazard function & $h_0(t) = \lambda$ & $h_0(t) = \lambda\gamma t^{\gamma-1}$ & $h_0(t) = \lambda\exp(\alpha t)$ \\ \hline
Cumulative hazard function & $H_0(t) = \lambda t$ & $H_0(t) = \lambda t^{\gamma}$ & $H_0(t) = \frac{\lambda}{\alpha}(\exp(\alpha t)-1)$ \\ \hline
Density function & $f_0(t) = \lambda\exp(-\lambda t)$ & $f_0(t) = \lambda\gamma t^{\gamma-1}\exp(-\lambda t^{\gamma})$ & $f_0(t) = \lambda\exp(\alpha t)\exp(\frac{\lambda}{\alpha}(1-\exp(\alpha t)))$ \\ \hline
Survival function & $S_0(t) = \exp(-\lambda t)$ & $S_0(t) = \exp(-\lambda t^{\gamma})$ & $f_0(t) = \exp(\frac{\lambda}{\alpha}(1-\exp(\alpha t)))$ \\ \hline
\end{tabular}
\end{center}

The exponential distribution with scale parameter $\lambda > 0$ has a constant hazard function for $t>0$. In practice, the assumption of a constant hazard function is only rarely tenable. A more general form of the hazard function is given by the Weibull distribution, which is characterized by two positive parameters, scale parameter $\lambda$ and the shape parameter $\gamma$. In the particular case where $\gamma = 1$ the hazard function reduces to that of the exponential distribution. For $\gamma>1$, the hazard function increases from $0\to\infty$ and for $0<\gamma<1$, it decreases monotonically from $\infty \to 0$.

Like the Weibull, the Gompertz distribution is characterized by two parameters. In the formulation shown in Table I, when $\alpha <0 \ (>0)$(we use $\gamma$ instead of $\alpha$ in r codes for simplicity), the hazard function decreases(increases) from $\exp(\alpha)$, and when $\alpha=0$, it reduces to the constant hazard function of an exponential distribution.

## Design of simulation settings
We conducted simulation studies to assess the performance of three survival models. In total, we created 7 simulation settings by mixing the event time generated from the three specified baseline hazard function: exponential, Weibull, and Gompertz hazard function. Then the generated data were fitted to exponential, Weibull, and Cox proportional hazard models. The parameters we applied in the models are constants: $\alpha = 0.5$, $\gamma = 1.5$, $\beta = -0.5$. Since the shape of Weibull distribution has great difference between $\gamma > 1$ and $\gamma < 1$, thus we simulated in scenarios where  $\gamma = 0.5$ and $\gamma = 1.5$, 

We simulated 500 data sets in each simulation setting. After running the models, a set of $\beta$ was extracted and used to calculate the mean bias, variance, and squared error. To evaluate the efficiency performances of models, we simulated data of different sample sizes: 20, 40, 60, 80, 100, 200, 400. Similarly, bias, variance, MSE are calculated. 

All the simulation processes were performed in R.

## Methods for generating data
The survival dataset contains treatment assignment, status indicator, and observed time. Treatment assignment variable $X_i$ is generated from a Bernoulli distribution with p = 0.5. By utilizing the inverse transformation method, we can obtain event time T: 

$T=H_{0}^{-1}\left(\frac{-\log (u)}{e^{x^{T} \beta}}\right)$, where $U \sim U(0,1)$

The followings are specific baseline hazard functions we applied:\

1. Survival time under Exponential distribution:
$T=-\frac{\log (u)}{\lambda e^{x^{T} \beta}}$

2. Survival time under Weibull distribution:
$T=\left(-\frac{\log (u)}{\lambda e^{x^{T} \beta}}\right)^{\frac{1}{\gamma}}$

3. Survival time under Gompertz distribution:
$T=\frac{1}{\alpha} \log \left(1-\frac{\alpha \log (u)}{\lambda e^{x^{T} \beta}}\right)$

We simulated survival data by the event time generated from the mixtures of three baseline distributions. 

Mixture of Exponential and Weibull distribution:
$$T=p *(-\frac{\log (u)}{\lambda e^{x^{T} \beta}}) + (1-p)*(-\frac{\log (u)}{\lambda e^{x^{T} \beta}})^{1 / \gamma}$$ 
We take values of p as 0, 0.5 and 1. When `p = 1`, event time is generated from exponential baseline; when `p = 0`, event time is generated from Weibull distribution. 

Mixture of Exponential and Gompertz distribution:
$$T=p *(-\frac{\log (u)}{\lambda e^{x^{T} \beta}}) + (1-p)*(\frac{1}{\alpha} \log \left(1-\frac{\alpha \log (u)}{\lambda e^{x^{T} \beta}}\right))$$
Similary, we take values of p as 0, 0.5 and 1. When `p = 1`, event time is generated from exponential baseline; when `p = 0`, event time is generated from Gompertz distribution. Finally, make event indicator variable by applying administrative censoring at t = 5.

By repeating each of the above simulation process 500 times, we get survival datasets with sample size ranging from 20 to 400. 

```{r, echo=FALSE}
simdat = function(n, p, lambda = 0.5, gamma = 1.5, eff = list()) {
  x1 = rbinom(n, 1, 0.5)
  u = runif(n)
  useExp = runif(n) < p 
  t =  useExp * (-log(u) / (lambda * exp(eff$b1 * x1))) + (1 - useExp) * (-log(u) / (lambda * exp(eff$b1 * x1)))^(1 / gamma)
  t[t < 1/365] = 1/365
  t[t == 1 / 365] = t[t == 1 / 365] + rnorm(length(t[t == 1 / 365]), 0, 1e-4)
  t = abs(t)
  e = as.numeric(t < 5)
  t = pmin(t, 5)
  name = paste("n =", n, ", p =", p, ", eff =", eff[[1]])
  return(tibble(name = name, time = t, event = e, x1 = x1))
}

simdat0.5 = function(n, p, lambda = 0.5, gamma = 0.5, eff = list()) {
  x1 = rbinom(n, 1, 0.5)
  u = runif(n)
  useExp = runif(n) < p 
  t =  useExp * (-log(u) / (lambda * exp(eff$b1 * x1))) + (1 - useExp) * (-log(u) / (lambda * exp(eff$b1 * x1)))^(1 / gamma)
  t[t < 1/365] = 1/365
  t[t == 1 / 365] = t[t == 1 / 365] + rnorm(length(t[t == 1 / 365]), 0, 1e-4)
  t = abs(t)
  e = as.numeric(t < 5)
  t = pmin(t, 5)
  name = paste("n =", n, ", p =", p, ", eff =", eff[[1]])
  return(tibble(name = name, time = t, event = e, x1 = x1))
}

simdat2 = function(n, p, lambda = 0.5, alpha = 0.5, eff = list()) {
  x1 = rbinom(n, 1, 0.5)
  u = runif(n)
  useExp = runif(n) < p 
  t =  useExp * (-log(u) / (lambda * exp(eff$b1 * x1))) + (1 - useExp) * (1/alpha) * log(1 - (alpha*log(u) / (lambda * exp(eff$b1 * x1))))
  t[t < 1/365] = 1/365
  t[t == 1 / 365] = t[t == 1 / 365] + rnorm(length(t[t == 1 / 365]), 0, 1e-4)
  t = abs(t)
  e = as.numeric(t < 5)
  t = pmin(t, 5)
  name = paste("n =", n, ", p =", p, ", eff =", eff[[1]])
  return(tibble(name = name, time = t, event = e, x1 = x1))
}
```


```{r, echo=FALSE}
fit_exp = function(df) {
  fit.exponential = survreg(Surv(time, event) ~ x1, dist = "exponential", data = df)
  return(as.numeric(-fit.exponential$coefficients[-1]))
}

fit_weibull = function(df) {
  fit.weibull <- survreg(Surv(time, event) ~ x1, dist = "weibull", data = df)
  return(as.numeric(-fit.weibull$coefficients[-1] / fit.weibull$scale))
}

fit_cox = function(df) {
  fit.cox <- coxph(Surv(time, event) ~ x1, data = df)
  return(as.numeric(fit.cox$coefficients))
}
```

## Selection of performance measure

We decided to use Average deviance, Variance and MSE to measure the performance of the 3 models we have got, which means whether the models can simulate the real hazard model most efficiently.

**Bias:**\
The bias is the average difference between the estimated treatment effects $\beta$ and the real $\beta$. The bias measure deviates the desired prediction of the learning algorithm from the true result, i.e., it portrays the fitting ability of the learning algorithm itself. The larger the bias, the greater the degree of difference between the estimated $\beta$ and the real $\beta$, and the less accuracy the simulation is.\
According to the definition, the model which have a smaller MBE might have a better predicting accuracy.\
In the data we generated, we assume $\beta_{1}=0.5$. So when calculating the average bias, we used the equation below to get the average bias of each model.\

*Exponential*: $$MBE=\sum\frac{\beta_{exp}-\beta_{1}}{n}=\sum\frac{\beta_{exp}-0.5}{n}$$\

*Weibull*: $$MBE=\sum\frac{\beta_{weibull}-\beta_{1}}{n}=\sum\frac{\beta_{weibull}-0.5}{n}$$\

*Cox*: $$MBE=\sum\frac{\beta_{cox}-\beta_{1}}{n}=\sum\frac{\beta_{cox}-0.5}{n}$$\

**Variance:**\
The variance is the average of the sum of the squares of the differences between the estimated $\beta$ and the real $\beta$. Variance measures the change in learning performance due to a change in the same size training set, i.e., it portrays the impact of data perturbations. The variance indicates how much the prediction function constructed by all models differs from the true function.\
Efficiency refers to two unbiased estimates of the same overall parameter where the estimate with smaller variance is more valid. According to the definition, the model which have a smaller variance will have a better predicting efficiency.
In the data we generated, we assume $\beta_{1}=0.5$. So when calculating the variance, we used the equation below to get the variance of each model.\

*Exponential*: $$variance= \sum\frac{(\beta_{exp}-\beta_{1})^2}{n-1}=\sum\frac{(\beta_{exp}-0.5)^2}{n-1}$$\

*Weibull*: $$variance= \sum\frac{(\beta_{weibull}-\beta_{1})^2}{n-1}=\sum\frac{(\beta_{weibull}-0.5)^2}{n-1}$$\

*Cox*: $$variance= \sum\frac{(\beta_{cox}-\beta_{1})^2}{n-1}=\sum\frac{(\beta_{cox}-0.5)^2}{n-1}$$\

Low bias with low variance is the effect we seek, when the predicted values are right on the bull's eye (closest to the true value) and are more concentrated (less variance).\

In the case of low bias and high variance, the predicted value basically falls around the true value, but it is very scattered, and the variance is larger, which means the stability of the model is not good enough.\

In the case of high bias and low variance, the predicted values are far from the true values, but the values are concentrated and the variance is small; the stability of the model is good, but the prediction accuracy is not high, and it is in the state of "inaccurate prediction as usual".\

When the bias is high and the variance is high, this is the last result we want to see. The model is not only inaccurate, but also unstable, and the predicted values are very different every time.\

**MSE**:
The mean squared error (MSE) in parameter estimation is the expected value of the squared difference between the estimated $\beta$ and the true $\beta$. It is defined as\
$$MSE(\beta)=var(\beta)+bias^2(\beta)$$\
According to the definition, the model which have a smaller MSE will have a better predicting performance in both accuracy and efficiency.
In the data we generated, we assume $\beta_{1}=0.5$. So when calculating the MSE, we used the equation below to get the MSE of each model.\

*Exponential*: $$MSE= \sum\frac{(\beta_{exp}-\beta_{1})^2}{n}=\sum\frac{(\beta_{exp}-0.5)^2}{n}$$\

*Weibull*: $$MSE= \sum\frac{(\beta_{weibull}-\beta_{1})^2}{n}=\sum\frac{(\beta_{weibull}-0.5)^2}{n}$$\

*Cox*: $$MSE= \sum\frac{(\beta_{cox}-\beta_{1})^2}{n}=\sum\frac{(\beta_{cox}-0.5)^2}{n}$$\

After getting the bias, variance and MSE of the 3 models when the size and the component of the data we generated is different, we made the spaghetti plot where the y value is the value of bias, variance or MSE and the x value is the size of the data.\
Based on the plot, we can find out which model is the most suitable model easily by compare the value of bias, variance and MSE.

\bigskip
## Results

Figure 1 shows the results of the simulation from mixtures of exponential distribution and Weibull distribution with sample sizes from 20 to 400. The mixing parameter p = 0 represents a full Weibull distribution with $\lambda$ = 0.5, $\gamma$ = 1.5; p = 1 represents a full exponential with $\lambda$ = 0.5($\gamma$ = 1); and p = 0.5 represents a half and half mixture. The true $\beta$ = -0.5 is lined as reference. We can see obviously from the plot that when sample size is large enough and the true distribution is Weibull, the Cox model and Weibull model outperform the exponential model, which always tends to overestimate the true $\beta$. With p increasing, the gaps tend to diminish gradually and when the true distribution is exponential, three models have similar performances.

```{r, echo=FALSE, warning=FALSE, fig.height=5}
param_grid = expand.grid(p = seq(0, 1, by = 0.5), n = c(20, 40, 60, 80, 100, 200, 400), rep = 1:500)

set.seed(2022)
sim_dat = param_grid %>% 
  mutate(
    data = map2(n, p, ~simdat(n = .x, p = .y, eff = list(b1 = -0.5)))
  ) %>% 
  mutate(
    b_exp = map_dbl(data, fit_exp),
    b_weibull = map_dbl(data, fit_weibull),
    b_cox = map_dbl(data, fit_cox)
  )

p.labs = c("p: 0(weibull)", "p: 0.5", "p: 1(exponential)")
names(p.labs) = c("0", "0.5", "1")

sim_dat %>% 
  ggplot() +
  geom_density(aes(x = b_exp, color = "darkred"), alpha = 0.6, size = .3) +
  geom_density(aes(x = b_weibull, color = "orange" ), size = .7) +
  geom_density(aes(x = b_cox, color = "blue"), alpha = 0.3, size = .3) +
  geom_vline(xintercept = -0.5) +
  facet_grid(n~p, labeller = labeller(p = p.labs)) +
  xlab(expression(paste("estimated ", beta))) +
  scale_colour_manual(name = 'fit methods', 
        values = c('blue' = 'blue','darkred' = 'darkred', 'orange' = 'orange'), labels = c('cox','exp', 'weibull')) +
  scale_x_continuous(breaks = c(-1, -0.5, 0), limits = c(-1.5, 0.5)) +
  labs(caption = "Figure 1") + 
  theme(plot.caption = element_text(hjust = 0.5, size = rel(1.2)))
```

\bigskip
Figure 2-4 show the bias, variance and MSE of each underlying distributions. From the bias plot, we can see that the exponential model has the largest bias, and the only case it has the highest prediction accuracy is when the true distribution close to a purely exponential. The big difference in performances of exponential model in different distributions shows that it is the least robust againt a misspecified distribution. On the other hand, the variance plot shows that the exponential model constantly has the lowest versatility. When looking at MSE, exponential model performs the best when sample sizes are relatively small; but for large sample sizes, the Cox model and Weibull model perform better. 

```{r, echo=FALSE, message=FALSE, fig.height=4, fig.width=6}
bias = sim_dat %>% 
  select(-data, -rep) %>% 
  mutate(
    b_exp = b_exp + 0.5, 
    b_weibull = b_weibull + 0.5,
    b_cox = b_cox + 0.5
  ) %>% 
  group_by(n, p) %>% 
  summarize(
    exp_bias = mean(b_exp),
    weibull_bias = mean(b_weibull),
    cox_bias = mean(b_cox)
  ) %>% 
  pivot_longer("exp_bias":"cox_bias", names_to = "fit_method", values_to = "bias") %>% 
  mutate(fit_method = as.factor(fit_method))

var = sim_dat %>% 
  select(-data, -rep) %>% 
  group_by(n, p) %>% 
  summarize(
    exp_var = var(b_exp),
    weibull_var = var(b_weibull),
    cox_var = var(b_cox)
  ) %>% 
  pivot_longer("exp_var":"cox_var", names_to = "fit_method", values_to = "variance") %>% 
  mutate(fit_method = as.factor(fit_method))


mse = sim_dat %>% 
  select(-data, -rep) %>%
  mutate(
    b_exp = (b_exp + 0.5)^2,
    b_weibull = (b_weibull + 0.5)^2,
    b_cox = (b_cox + 0.5)^2
  ) %>% 
  group_by(n, p) %>% 
  summarize(
    exp_mse = mean(b_exp),
    weibull_mse = mean(b_weibull),
    cox_mse = mean(b_cox)
  ) %>% 
  pivot_longer("exp_mse":"cox_mse", names_to = "fit_method", values_to = "MSE") %>% 
  mutate(fit_method = as.factor(fit_method)) 
  

ggplot(bias, aes(x = n, y = bias, color = fit_method)) +
  geom_point(size = .3) +
  geom_line(size = .3) +
  #scale_y_log10() +
  scale_x_log10() +
  facet_grid("p", labeller = label_both) +
  xlab("sample size") +
  labs(caption = "Figure 2") + 
  theme(plot.caption = element_text(hjust = 0.5, size = rel(1.2))) +
  geom_hline(yintercept = 0)

ggplot(var, aes(x = n, y = variance, color = fit_method)) +
  geom_point(size = .3) +
  geom_line(size = .3) +
  scale_y_log10() +
  scale_x_log10() +
  facet_grid("p", labeller = label_both) +
  xlab("sample size") +
  labs(caption = "Figure 3") + 
  theme(plot.caption = element_text(hjust = 0.5, size = rel(1.2)))

ggplot(mse, aes(x = n, y = MSE, color = fit_method)) +
  geom_point(size = .3) +
  geom_line(size = .3) +
  scale_y_log10() +
  scale_x_log10() +
  facet_grid("p", labeller = label_both) +
  xlab("sample size") +
  labs(caption = "Figure 4") + 
  theme(plot.caption = element_text(hjust = 0.5, size = rel(1.2)))
```

Figure 5 shows the results of the simulation from mixtures of exponential and Weibull distribution with $\gamma$ = 0.5. It is the similar case as Figure 1, only this time the exponential model tends to underestimate the true $\beta$.

```{r, echo=FALSE, warning=FALSE, fig.height=4.3}
param_grid0.5 = expand.grid(p = seq(0, 1, by = 0.5), n = c(20, 40, 60, 80, 100, 200, 400), rep = 1:500)

set.seed(2022)
sim_dat0.5 = param_grid0.5 %>% 
  mutate(
    data = map2(n, p, ~simdat0.5(n = .x, p = .y, eff = list(b1 = -0.5)))
  ) %>% 
  mutate(
    b_exp = map_dbl(data, fit_exp),
    b_weibull = map_dbl(data, fit_weibull),
    b_cox = map_dbl(data, fit_cox)
  )

sim_dat0.5 %>% 
  ggplot() +
  geom_density(aes(x = b_exp, color = "darkred"), alpha = 0.6, size = .3) +
  geom_density(aes(x = b_weibull, color = "orange" ), size = .7) +
  geom_density(aes(x = b_cox, color = "blue"), alpha = 0.3, size = .3) +
  geom_vline(xintercept = -0.5) +
  facet_grid(n~p, labeller = labeller(p = p.labs)) +
  xlab(expression(paste("estimated ", beta))) +
  scale_colour_manual(name = 'fit methods', 
        values = c('blue' = 'blue','darkred' = 'darkred', 'orange' = 'orange'), labels = c('cox','exp', 'weibull')) +
  scale_x_continuous(breaks = c(-1, -0.5, 0), limits = c(-1.5, 0.5)) +
  labs(caption = "Figure 5") + 
  theme(plot.caption = element_text(hjust = 0.5, size = rel(1.2)))
```

Figure 6-8 show some slightly different results from the previous simulation. In this case the exponential model has the poorest performance in both accuracy and efficiancy, regardless of sample size.

```{r, echo=FALSE, message=FALSE, fig.height=3.6, fig.width=6}
bias0.5 = sim_dat0.5 %>% 
  select(-data, -rep) %>% 
  mutate(
    b_exp = b_exp + 0.5, 
    b_weibull = b_weibull + 0.5,
    b_cox = b_cox + 0.5
  ) %>% 
  group_by(n, p) %>% 
  summarize(
    exp_bias = mean(b_exp),
    weibull_bias = mean(b_weibull),
    cox_bias = mean(b_cox)
  ) %>% 
  pivot_longer("exp_bias":"cox_bias", names_to = "fit_method", values_to = "bias") %>% 
  mutate(fit_method = as.factor(fit_method))

var0.5 = sim_dat0.5 %>% 
  select(-data, -rep) %>% 
  group_by(n, p) %>% 
  summarize(
    exp_var = var(b_exp),
    weibull_var = var(b_weibull),
    cox_var = var(b_cox)
  ) %>% 
  pivot_longer("exp_var":"cox_var", names_to = "fit_method", values_to = "variance") %>% 
  mutate(fit_method = as.factor(fit_method))


mse0.5 = sim_dat0.5 %>% 
  select(-data, -rep) %>%
  mutate(
    b_exp = (b_exp + 0.5)^2,
    b_weibull = (b_weibull + 0.5)^2,
    b_cox = (b_cox + 0.5)^2
  ) %>% 
  group_by(n, p) %>% 
  summarize(
    exp_mse = mean(b_exp),
    weibull_mse = mean(b_weibull),
    cox_mse = mean(b_cox)
  ) %>% 
  pivot_longer("exp_mse":"cox_mse", names_to = "fit_method", values_to = "MSE") %>% 
  mutate(fit_method = as.factor(fit_method)) 
  

ggplot(bias0.5, aes(x = n, y = bias, color = fit_method)) +
  geom_point(size = .3) +
  geom_line(size = .3) +
  #scale_y_log10() +
  scale_x_log10() +
  facet_grid("p", labeller = label_both) +
  xlab("sample size") +
  labs(caption = "Figure 6") + 
  theme(plot.caption = element_text(hjust = 0.5, size = rel(1.2))) +
  geom_hline(yintercept = 0)

ggplot(var0.5, aes(x = n, y = variance, color = fit_method)) +
  geom_point(size = .3) +
  geom_line(size = .3) +
  scale_y_log10() +
  scale_x_log10() +
  facet_grid("p", labeller = label_both) +
  xlab("sample size") +
  labs(caption = "Figure 7") + 
  theme(plot.caption = element_text(hjust = 0.5, size = rel(1.2)))

ggplot(mse0.5, aes(x = n, y = MSE, color = fit_method)) +
  geom_point(size = .3) +
  geom_line(size = .3) +
  scale_y_log10() +
  scale_x_log10() +
  facet_grid("p", labeller = label_both) +
  xlab("sample size") +
  labs(caption = "Figure 8") + 
  theme(plot.caption = element_text(hjust = 0.5, size = rel(1.2)))
```

Finally, Figure 9 shows the results of the simulation from a Gompertz distribution, with different proportions of data contaminated by an exponential distribution. At large sample sizes, the Cox model performs the best apparently, because the true distribution is neither exponential nor Weibull. This shows that the Cox model has the highest robustness against misspecified baseline hazard functions, than followed by Weibull, and the most strict exponential model performs poorest when the true distribution does not match.

```{r, echo=FALSE, warning=FALSE, fig.height=4.2}
param_grid2 = expand.grid(p = seq(0, 0.5, by = 0.25), n = c(20, 40, 60, 80, 100, 200, 400), rep = 1:500)

set.seed(2022)
sim_dat2 = param_grid2 %>% 
  mutate(
    data = map2(n, p, ~simdat2(n = .x, p = .y, eff = list(b1 = -0.5)))
  ) %>% 
  mutate(
    b_exp = map_dbl(data, fit_exp),
    b_weibull = map_dbl(data, fit_weibull),
    b_cox = map_dbl(data, fit_cox)
  )

p.labs2 = c("p: 0(Gompertz)", "p: 0.25", "p: 0.5")
names(p.labs2) = c("0", "0.25", "0.5")

sim_dat2 %>% 
  ggplot() +
  geom_density(aes(x = b_exp, color = "darkred"), alpha = 0.6, size = .3) +
  geom_density(aes(x = b_weibull, color = "orange" ), size = .5) +
  geom_density(aes(x = b_cox, color = "blue"), alpha = 0.3, size = .3) +
  geom_vline(xintercept = -0.5) +
  facet_grid(n~p, labeller = labeller(p = p.labs2)) +
  xlab(expression(paste("estimated ", beta))) +
  scale_colour_manual(name = 'fit methods', 
        values = c('blue' = 'blue','darkred' = 'darkred', 'orange' = 'orange'), labels = c('cox','exp', 'weibull')) +
  scale_x_continuous(breaks = c(-1, -0.5, 0), limits = c(-1.5, 0.5)) +
  labs(caption = "Figure 9") + 
  theme(plot.caption = element_text(hjust = 0.5, size = rel(1.2)))
```

Figure 10-12 show that the exponential is still the most biased one among the three models, and it keeps the highest efficiency with the smallest variance, followed by the Weibull model-but when the true distribution becomes more mixed up, the Cox model tends to outperform the Weibull model. The MSE plot shows that with small sample sizes, the exponential model has good prediction performance; while with large sample sizes, the Cox model has better prediction performance. 

```{r, echo=FALSE, message=FALSE, fig.height=3.6, fig.width=6}
bias2 = sim_dat2 %>% 
  select(-data, -rep) %>% 
  mutate(
    b_exp = b_exp + 0.5, 
    b_weibull = b_weibull + 0.5,
    b_cox = b_cox + 0.5
  ) %>% 
  group_by(n, p) %>% 
  summarize(
    exp_bias = mean(b_exp),
    weibull_bias = mean(b_weibull),
    cox_bias = mean(b_cox)
  ) %>% 
  pivot_longer("exp_bias":"cox_bias", names_to = "fit_method", values_to = "bias") %>% 
  mutate(fit_method = as.factor(fit_method))

var2 = sim_dat2 %>% 
  select(-data, -rep) %>% 
  group_by(n, p) %>% 
  summarize(
    exp_var = var(b_exp),
    weibull_var = var(b_weibull),
    cox_var = var(b_cox)
  ) %>% 
  pivot_longer("exp_var":"cox_var", names_to = "fit_method", values_to = "variance") %>% 
  mutate(fit_method = as.factor(fit_method))


mse2 = sim_dat2 %>% 
  select(-data, -rep) %>%
  mutate(
    b_exp = (b_exp + 0.5)^2,
    b_weibull = (b_weibull + 0.5)^2,
    b_cox = (b_cox + 0.5)^2
  ) %>% 
  group_by(n, p) %>% 
  summarize(
    exp_mse = mean(b_exp),
    weibull_mse = mean(b_weibull),
    cox_mse = mean(b_cox)
  ) %>% 
  pivot_longer("exp_mse":"cox_mse", names_to = "fit_method", values_to = "MSE") %>% 
  mutate(fit_method = as.factor(fit_method)) 
  

ggplot(bias2, aes(x = n, y = bias, color = fit_method)) +
  geom_point(size = .3) +
  geom_line(size = .3) +
  #scale_y_log10() +
  scale_x_log10() +
  facet_grid("p", labeller = label_both) +
  xlab("sample size") +
  labs(caption = "Figure 10") + 
  theme(plot.caption = element_text(hjust = 0.5, size = rel(1.2))) +
  geom_hline(yintercept = 0)

ggplot(var2, aes(x = n, y = variance, color = fit_method)) +
  geom_point(size = .3) +
  geom_line(size = .3) +
  scale_y_log10() +
  scale_x_log10() +
  facet_grid("p", labeller = label_both) +
  xlab("sample size") +
  labs(caption = "Figure 11") + 
  theme(plot.caption = element_text(hjust = 0.5, size = rel(1.2)))

ggplot(mse2, aes(x = n, y = MSE, color = fit_method)) +
  geom_point(size = .3) +
  geom_line(size = .3) +
  scale_y_log10() +
  scale_x_log10() +
  facet_grid("p", labeller = label_both) +
  xlab("sample size") +
  labs(caption = "Figure 12") + 
  theme(plot.caption = element_text(hjust = 0.5, size = rel(1.2)))
```

## Conclusions and discussion

In conclusion, the exponential model is the most restrictive one, with only one parameter $\lambda$, while the Weibull has an additional $\gamma$ and the Cox does not specify a certain baseline hazard function, being the most flexible and general model. Thus it is natural that the exponential model tends to have higher bias and lower variance, or we can say, lower prediction accuracy and higher efficiency compared to the other two models. Also because of the lack of freedom, the robustness against misspecified baseline hazard functions of exponential models is the weakest. On the contrary, the Cox model are the most robust, and can fit to any kinds of underlying distribution smoothly, especially when sample size is large.

In reality, it is hard to figure out the true distribution of data. For general users, we recommend to choose a model base on the sample size. When the sample size is relatively small, an exponential model will perform the best because it is the stablest and least likely to get wild - even if you do not have enough observations, it has the largest probability to give you a quite reliable and meaningful estimate. On the other hand, if fortunately you have a large sample size, the Cox model will potentially give the best performances.

Interestingly, we find that in the simulation where $\gamma$ = 0.5 in Weibull distribution, the exponential model tends to underestimate the true $\beta$, while it tends to overestimate the true $\beta$ when $\gamma$ = 1.5. We know that for $\gamma$ > 1, the hazard function increases monotonously and for $\gamma$ < 1, the hazard function decreases monotonously. The exponential model is a special case when $\gamma$ = 1 and hazard is a constant number. Figure 13-14 show the different Weibull curves for $\gamma$ = 1.5 and $\gamma$  = 0.5 respectively. This shows that when the baseline function is lower than the true model, the exponential model will have a higher estimate of $\beta$ to complement for the gap, which also indicates that it is not quite robust against misspecified baseline hazard functions.

```{r, echo=FALSE, fig.height=3.6, fig.width=4.5}
exp_haz <- function(t, lambda = 0.5) lambda * 1 * t^0
weibull_haz <- function(t, lambda = 0.5, gamma = 1.5) lambda * gamma * t^(gamma - 1)
gompertz_haz <- function(t, alpha = 0.5, lambda = 0.1) lambda * exp(alpha* t)
curve(exp_haz, from = 0, to = 5, lty = 1, ylim = c(0, 2), ylab = expression(h[0](t)), xlab = "Follow-up time t")
curve(weibull_haz, from = 0, to = 5, lty = 2, add = TRUE)
curve(gompertz_haz, from = 0, to = 5, lty = 3, add = TRUE )
legend(x = "topleft", lty = 1:3, legend = c("Exponential baseline hazard", "Weibull baseline hazard","Gompertz baseline harzard"), bty = "n")
title(sub = "Figure 13")
```

``` {r, echo=FALSE, fig.height=3.6, fig.width=4.5}
exp_haz <- function(t, lambda = 0.5) lambda * 1 * t^0
weibull_haz <- function(t, lambda = 0.5, gamma = 0.5) lambda * gamma * t^(gamma - 1)
gompertz_haz <- function(t, alpha = 0.5, lambda = 0.1) lambda * exp(alpha* t)
curve(exp_haz, from = 0, to = 5, lty = 1, ylim = c(0, 2), ylab = expression(h[0](t)), xlab = "Follow-up time t")
curve(weibull_haz, from = 0, to = 5, lty = 2, add = TRUE)
curve(gompertz_haz, from = 0, to = 5, lty = 3, add = TRUE )
legend(x = "topleft", lty = 1:3, legend = c("Exponential baseline hazard", "Weibull baseline hazard","Gompertz baseline harzard"), bty = "n")
title(sub = "Figure 14")
```

This simulation study has some limitations. We only test some certain values of our parameters so the generalizability of our conclusion is still questioned. For further simulation, we could try more combinations of $\beta$, $\lambda$, $\gamma$, $\alpha$, and p. In addition, we could take into consideration even more types of other possible baseline hazard functions, like log-logistic distribution, gamma distribution, log-normal distribution etc. and the mixtures of them. We could test for different time censoring as well. 

## Acknowledgement

All members contributed equally to the project. 

## Reference

Austin PC. Generating survival times to simulate Cox proportional hazards models with time-varying covariates [published correction appears in Stat Med. 2013 Mar 15;32(6):1078]. Stat Med. 2012;31(29):3946-3958. doi:10.1002/sim.5452

Bender, R., Augustin, T., & Blettner, M. (2005). Generating survival times to simulate Cox proportional hazards models. Statistics in medicine, 24(11), 1713–1723. https://doi.org/10.1002/sim.2059

Moore, D. F. (2016). Applied survival analysis using R (p. 234). New York, NY: Springer.